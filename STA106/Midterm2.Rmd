---
title: "JONG WOOK CHOE"
author: "STA 106, Winter 2021"
date: "Maxime Pouokam" 
output: pdf_document
---

\newpage

```{r, echo=FALSE, warning=FALSE}
h.data = read.csv("/Users/Bosco/Desktop/WINTER2021/STA 106/data/Helicopter.csv")
nt = nrow(h.data)
a=length(unique(h.data$Shift))
h.model = lm(Count ~ Shift, data= h.data)
h.data$ei = h.model$residuals
p.valSW0 = shapiro.test(h.model$residuals)$p.val
suppressMessages(library(car))
the.BFtest = leveneTest(ei~ Shift, data=h.data, center=median)
p.valBF0 = the.BFtest[[3]][1]
t.cutoff= qt(1-0.01,nt -a)
rij = rstandard(h.model)
outliers = which(abs(rij) > t.cutoff)
new.data = h.data[-outliers,]
new.model = lm(Count ~ Shift,data = new.data)
p.valSWN = shapiro.test(new.model$residuals)$p.val
the.BFtest = leveneTest(ei~ Shift, data=new.data, center=median)
p.valBFN = the.BFtest[[3]][1]
suppressMessages(library(EnvStats))
lambda = boxcox(h.model,objective.name = "Shapiro-Wilk",optimize = TRUE)$lambda
newY = (h.data$Count^lambda-1)/(lambda)
T.data = data.frame(Count = newY, Shift = h.data$Shift)
T.model = lm(Count ~ Shift, data= T.data)
T.data$ei = T.model$residuals
p.valSWT = shapiro.test(T.model$residuals)$p.val
the.BFtest = leveneTest(ei~ Shift, data=T.data, center=median)
p.valBFT = the.BFtest[[3]][1]

lambda1 = boxcox(new.model,objective.name = "Shapiro-Wilk",optimize = TRUE)$lambda
newYY = (new.data$Count^lambda1-1)/(lambda1)
TT.data = data.frame(Count = newYY, Shift = new.data$Shift)
TT.model = lm(Count ~ Shift, data= TT.data)
TT.data$ei = TT.model$residuals
p.valSWTT = shapiro.test(TT.model$residuals)$p.val
the.BFtest = leveneTest(ei~ Shift, data=TT.data, center=median)
p.valBFTT = the.BFtest[[3]][1]

SFA.n.pval = anova(new.model)[1,5]
SFA.T.pval = anova(T.model)[1,5]
SFA.TT.pval = anova(TT.model)[1,5]
```

#### Part I: Report for Topic I

a. The dataset "Helicopter.csv" is about different amount of helicopter requested to the sheriff's office in different times of day. The goal is to diagnose and transform the data in order to find best fits of the ANOVA model.

b. The plot follow:

```{r, echo=FALSE, warning=FALSE}
par(mfrow = c(1, 3))
plot(h.model$fitted.values, h.model$residuals, main = "Errors vs. Group Means",xlab = "Group Means",ylab = "Errors",pch = 19)
abline(h = 0,col = "purple")
qqnorm(h.model$residuals)
qqline(h.model$residuals)
hist(h.model$residuals,main = "Histogram of errors",xlab = "e_ij",pch = 19)
```

Diagnostic plot of the original data shows some outliers which cause non-normality and unequal variance from the vertical spread by group. Non-normality and non-constant variance are the violation of ANOVA assumption. To be more precise Shapiro-Wilks test and Brown-Forstyhe test checks normality and variance consistency. The null and lternative of both test are following,

* $H_0:$ The errors are normally distributed    vs. $H_A:$ The errors are not normally distributed. 
* $H_0:\sigma^2_I=\sigma^2_{II}=\sigma^2_{III}$ vs. $H_A:$ At least one group variance is unequal

To follow assumption of ANOVA the test should fail to reject the null and conclude the errors are normally distributed and all variances are constant. The p-value for Shapiro-Wilks is `r p.valSW0`, and for Brown-Forstyhe is `r p.valBF0`. The p-values are small enough to conclude that the values are smaller than $\alpha$. As a result the test reject the null and conclude the errors are not normal and the variances are also unequal. 


c. The plot follow:

```{r, echo=FALSE, warning=FALSE}
par(mfrow = c(1, 3))
qqnorm(new.model$residuals,main = "Outliers Removed, nt=77")
qqline(new.model$residuals)
qqnorm(T.model$residuals,main = "Transformed data, nt=80")
qqline(T.model$residuals)
qqnorm(TT.model$residuals,main = "Outliers Removed and Transformed data, nt=77",cex.main=0.88)
qqline(TT.model$residuals)
```

Non-normality and non-constant variance are the violation of ANOVA assumption. To find best fits of the ANOVA model data can remove outlier, transform Y, or both. All Quantile-Quantile plots has points that more closely follow the ideal line. Overall fit has improved from the original. 

* The p-value of outlier removed data for Shapiro-Wilks is `r p.valSWN`, and for Brown-Forstyhe is `r p.valBFN`. 

The p-values are still small and conclude the errors are not normal and the variances are also unequal. 

* The p-value of transformed data for Shapiro-Wilks is `r p.valSWT`, and for Brown-Forstyhe is `r p.valBFT`. 

Now p-value is larger than with any reasonable value of $\alpha$. Therefore fail to reject the null and conclude the errors are normally distributed and all variances are constant. It is same with outliers removed and transformed data. 

* The p-value for Shapiro-Wilks is `r p.valSWTT`, and for Brown-Forstyhe is `r p.valBFTT`. 

It also fail to reject the null and conclude the errors are normally distributed and all variances are constant. Therefore transformed data and outliers removed and transformed data both fits for ANOVA model.

d.

Transforming the data is critical because of its downside. It can often alleviate non-normality or non-constant variance. The downside is that interpretation can be difficult. Also it can not be reversed. However outliers is only 3.75% of the data, removing outliers are unnecessary. Therefore the transformed data is the best fits of the ANOVA model. As a result, it may be difficult to interpret the transformed data but to see clear result it is better to use transformed data set for ANOVA.

```{r, echo=FALSE, warning=FALSE}
s.data = read.csv("/Users/Bosco/Desktop/WINTER2021/STA 106/data/Salary.csv")
group.means = by(s.data$Annual,s.data$Prof,mean)
group.sds = by(s.data$Annual,s.data$Prof,sd)
group.nis = by(s.data$Annual,s.data$Prof,length)
tthe.summary = rbind(group.means,group.sds,group.nis)
tthe.summary = round(tthe.summary,digits = 4)
colnames(tthe.summary) = names(group.means)
rownames(tthe.summary) = c("Means","Std. Dev","Sample Size")

group.means = by(s.data$Annual,s.data$Region,mean)
group.sds = by(s.data$Annual,s.data$Region,sd)
group.nis = by(s.data$Annual,s.data$Region,length)
the.summary = rbind(group.means,group.sds,group.nis)
the.summary = round(the.summary,digits = 4)
colnames(the.summary) = names(group.means)
rownames(the.summary) = c("Means","Std. Dev","Sample Size")

find.means = function(the.data,fun.name = mean){
a = length(unique(the.data[,2]))
b = length(unique(the.data[,3]))
means.A = by(the.data[,1], the.data[,2], fun.name)
means.B = by(the.data[,1],the.data[,3],fun.name)
means.AB = by(the.data[,1],list(the.data[,2],the.data[,3]),fun.name)
MAB = matrix(means.AB,nrow = b, ncol = a, byrow = TRUE)
colnames(MAB) = names(means.A)
rownames(MAB) = names(means.B)
MA = as.numeric(means.A)
names(MA) = names(means.A)
MB = as.numeric(means.B)
names(MB) = names(means.B)
results = list(A = MA, B = MB, AB = MAB)
return(results)
}
a = length(unique(s.data[,2]))
b = length(unique(s.data[,3]))
n = nrow(s.data)
AB =lm( Annual ~ Prof * Region,data = s.data)
A.B = lm( Annual~  Prof + Region, data = s.data)
A = lm(Annual ~ Prof, data = s.data)
B = lm(Annual ~ Region, data = s.data)
N = lm(Annual ~ 1, data = s.data)
AllM = list(AB, A.B, A, B, N)
SSEs = round(sapply(AllM, function(i) sum(i$residual^2)),2)
SSEF = SSEs[1]
SSER = SSEs[2]
df.F= n - a*b
df.R = n -(a-1)-(b-1)-1
FN = round((SSER - SSEF)/(df.R - df.F),4)
FD = round(SSEF/df.F,4)
Fs = round(FN/FD,4)

results = anova(A.B,AB)
p.val = results[2,6]

s.model = lm(Annual ~ Prof*Region, data= s.data)
s.data$ei = s.model$residuals
p.valSW = shapiro.test(s.model$residuals)$p.val
suppressMessages(library(car))
the.BFtest = leveneTest(ei~ Prof*Region, data=s.data, center=median)
p.valBF = the.BFtest[[3]][1]

Partial.R2 = function(small.model,big.model){
  SSE1 = sum(small.model$residuals^2)
  SSE2 = sum(big.model$residuals^2)
  PR2 = (SSE1 - SSE2)/SSE1
  return(PR2)
}
RAB=Partial.R2(A.B,AB)
RA=Partial.R2(A,A.B)
RB=Partial.R2(B,A.B)
RA0=Partial.R2(N,A)
RB0=Partial.R2(N,B)

scary.CI = function(the.data,MSE,equal.weights = TRUE,multiplier,group,cs){
if(sum(cs) != 0 & sum(cs !=0 ) != 1){
return("Error - you did not input a valid contrast")
}else{
the.means = find.means(the.data)
the.ns =find.means(the.data,length)
nt = nrow(the.data)
a = length(unique(the.data[,2]))
b = length(unique(the.data[,3]))
if(group =="A"){
if(equal.weights == TRUE){
a.means = rowMeans(the.means$AB)
est = sum(a.means*cs)
mul = rowSums(1/the.ns$AB)
SE = sqrt(MSE/b^2 * (sum(cs^2*mul)))
N = names(a.means)[cs!=0]
CS = paste("(",cs[cs!=0],")",sep = "")
fancy = paste(paste(CS,N,sep =""),collapse = "+")
names(est) = fancy
} else{
a.means = the.means$A
est = sum(a.means*cs)
SE = sqrt(MSE*sum(cs^2*(1/the.ns$A)))
N = names(a.means)[cs!=0]
CS = paste("(",cs[cs!=0],")",sep = "")
fancy = paste(paste(CS,N,sep =""),collapse = "+")
names(est) = fancy
}
}else if(group == "B"){
if(equal.weights == TRUE){
b.means = colMeans(the.means$AB)
est = sum(b.means*cs)
mul = colSums(1/the.ns$AB)
SE = sqrt(MSE/a^2 * (sum(cs^2*mul)))
N = names(b.means)[cs!=0]
CS = paste("(",cs[cs!=0],")",sep = "")
fancy = paste(paste(CS,N,sep =""),collapse = "+")
names(est) = fancy
} else{
b.means = the.means$B
est = sum(b.means*cs)
SE = sqrt(MSE*sum(cs^2*(1/the.ns$B)))
N = names(b.means)[cs!=0]
CS = paste("(",cs[cs!=0],")",sep = "")
fancy = paste(paste(CS,N,sep =""),collapse = "+")
names(est) = fancy
}
} else if(group == "AB"){
est = sum(cs*the.means$AB)
SE = sqrt(MSE*sum(cs^2/the.ns$AB))
names(est) = "someAB"
}
the.CI = est + c(-1,1)*multiplier*SE
results = c(est,the.CI)
names(results) = c(names(est),"lower bound","upper bound")
return(results)
}
}
find.mult = function(alpha,a,b,dfSSE,g,group){
if(group == "A"){
Tuk = round(qtukey(1-alpha,a,dfSSE)/sqrt(2),3)
Bon = round(qt(1-alpha/(2*g), dfSSE ) ,3)
Sch = round(sqrt((a-1)*qf(1-alpha, a-1, dfSSE)),3)
}else if(group == "B"){
Tuk = round(qtukey(1-alpha,b,dfSSE)/sqrt(2),3)
Bon = round(qt(1-alpha/(2*g), dfSSE ) ,3)
Sch = round(sqrt((b-1)*qf(1-alpha, b-1, dfSSE)),3)
}else if(group == "AB"){
Tuk = round(qtukey(1-alpha,a*b,dfSSE)/sqrt(2),3)
Bon = round(qt(1-alpha/(2*g), dfSSE ) ,3)
Sch = round(sqrt((a*b-1)*qf(1-alpha, a*b-1, dfSSE)),3)
}
results = c(Bon, Tuk,Sch)
names(results) = c("Bonferroni","Tukey","Scheffe")
return(results)
}
SSE = sum(s.model$residuals^2)
MSE = SSE/(n-a*b)
Tuk = find.mult(alpha = 0.01, a = 3, b = 2, dfSSE = 120 - 2*3, g = 1, group = "B")[2]
B.cs = c(1,-1)
CI=scary.CI(s.data,MSE,equal.weights = FALSE,Tuk,"B",B.cs)[2:3]
```

#### Part II: Report for Topic II

I. 

This data is interesting to see the relationship between salary, professionally, and the region. But what intriguing most is that the data shows how much people who are related to the field could expected in the future. "Annual" the subjects annual salary in thousands of dollars will be the response variable of the study. There are two factor "Prof" and "Region". First factor is about title of the subjects DS(Data Scientist), SE(Software Engineer), and BE(Bioinformatics). Second factor is about the place SF for San Francisco and S for Seattle. Following study will see the interaction and individual factor effects to the model and find the best distribution model. Through diagnostics test the data will be checked to see whether ANOVA assumptions hold. The best model data will be checked through test statistic and reasonable confidence level. Lastly with proper confidence interval it will see which factor level has significant difference. 

II. The plot follow:

```{r, echo=FALSE, warning=FALSE}
suppressMessages(library(gridExtra))
suppressMessages(library(ggplot2))
a=ggplot(s.data, aes(x = Annual)) + geom_histogram(binwidth = 10,color = "black",fill = "white") +
facet_grid(Prof ~.) +ggtitle("Annual by number of Prof")
b=ggplot(s.data, aes(x = Annual)) + geom_histogram(binwidth = 10,color = "black",fill = "white") +
facet_grid(Region ~.) +ggtitle("Annual by number of Region")
grid.arrange(a,b, nrow=1,ncol=2)
par(mfrow = c(2, 2),mar=c(2,0.5,2,0.5))
boxplot(Annual ~ Prof,data=s.data, main="box plot by Prof")
boxplot(Annual ~ Region,data=s.data, main="box plot by Region")
suppressMessages(require(graphics))
interaction.plot(x.factor = s.data$Prof, trace.factor = s.data$Region, response = s.data$Annual, type = "b", ylab = "Annual", xlab = "Prof", pch=c(2,4), col = c(2,4), lty = 1, lwd = 2, trace.label = "Region", xpd = FALSE)
interaction.plot(x.factor = s.data$Region, trace.factor = s.data$Prof, response = s.data$Annual, type = "b", ylab = "Annual", xlab = "Prof", pch=c(2,4), col = c(2,4), lty = 1, lwd = 2, trace.label = "Region", xpd = FALSE)
```

Boxes from the box plot by job are in various heights, and their midpoints differ significantly. Data scientist tend to have larger trend than other factor level. The medians of each box plot by region appear to be approximately equal but San Francisco tend to have larger trend than Seattle. Interaction effect is when a combination of two factor levels causes an additional effect on the out come of response variable. Interaction plots with approximately parallel lines is said to suggest no interaction between factors.

sample means and standard deviations are following,

\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & BE & DS & SE \\ 
  \hline
means & 81.0 & 115.1 & 102.9 \\ 
std. dev. & 9.7 & 13.7 & 13.2 \\ 
sample size & 40 & 40 & 40 \\ 
   \hline
\end{tabular}


\begin{tabular}{rrrrr}
  \hline
 & S & SF \\ 
  \hline
means & 95.9 & 103.5 \\ 
std. dev. & 17.4 & 19.3 \\ 
sample size & 60 & 60 \\ 
   \hline
\end{tabular}
\end{center}

Since sample size are equally weighted finding confidence interval for all sample means, assuming all factor levels or treatments are equally weighted. 

III.


Two Factor ANOVA with interaction model comes with following assumptions

* All subjects are randomly sampled.
* All levels of factor A are independent.
* All levels of factor B are independent.
* $\epsilon_{ijk}$~N(0,$\sigma^2_{\epsilon}$)
* $(\gamma\delta)_{ij}=\mu_{ij}+\gamma_i+\delta_j-\mu_{..}$

Through diagnostic test the data can be checked whether Two Factor ANOVA assumptions hold. The p-value for Shapiro-Wilks is `r p.valSW`, and for Brown-Forstyhe is `r p.valBF`. The p-value from the test result is larger than any reasonable value of $\alpha$. Therefore fail to reject the null and conclude the errors are normally distributed and all variances are constant. Since the assumptions are not violated there are no need for transformation.

IV.

First to test interactions there are

"Full Model" : $Y_{ijk}=\mu_{..}++\gamma_i+\delta_j+(\gamma\delta)_{ij}+\epsilon_{ijk}$
with d.f{SSE}=$n_T-ab$

"Reduced Model" : $Y_{ijk}=\mu_{..}+\gamma_i+\delta_j+\epsilon_{ijk}$
with d.f{SSE}=$n_T-a-b+1$

Now state the null and alternative hypothesis following,

$H_0$ : All $(\gamma\delta)_{ij}$ is 0 vs. $H_A$ : At least one $(\gamma\delta)_{ij}$ is not 0

The test-statistic is :$F_s=[\frac{SSE_R-SSE_F}{dfSSE_R-dfSSE_F}]*\frac{1}{MSE_F}$=`r Fs` 

By the F-table,   p-value < 0.05 . With smallest $\alpha$ respect to 0.01, p-value > $\alpha$ and conclude interaction term between profession and region is not needed. In other words, interaction effects can be removed from the model. In order to support answer conditional $R^2$ shows the proportion of reduction in error when using the "Full" model instead of the "Reduced" model. If the value is small, it typically suggests interaction effects are not important. 

When SSE values for various models follow,

\begin{center}
\begin{tabular}{rrrrrr}
  \hline
 Model & AB & (A + B) & A & B & Empty/Null \\ 
  \hline
SSE & 15252.93 & 16058.34 & 17764.09 &  39872.94 & 41578.69 \\ 
   \hline
\end{tabular}
\end{center}

$R^2${$M_F|M_R$}=$\frac{SSE_R-SSE_F}{SSE_R}$=`r RAB`.  Adding interaction effects between profession and region reduces the error of a model with factor effects of profession and region by 5%. Since it is a small reduction in error, it suggests that an interaction effect is not important. Addition to that $R^2${$A|.$}=`r RA0` and $R^2${$B|.$}=`r RB0`. It tells adding profession factor reduces the overall error of the empty model more than adding region factor. Therefore profession factor is more important to the model. However it doesn't mean region facotr has no meaning to the model. Based on the 99% confidence interval for $\mu_{.S}-\mu_{.SF}$ is (`r CI`). Interval does not contain zero suggest significant factor region effects and difference in the group averages for factor region.

Since there are no interaction effects but there are profession and region factor effect, a model with no interaction with both factor profession and region effects is the best model. In other words, the model is following,

\begin{center}
$Y_{ijk}=\mu_{..}+\gamma_i+\delta_j+\epsilon_{ijk}$
\end{center}
with constraints $\sum_i\gamma_i=0,\sum_j\delta_j=0$, and with assumptions

* All subjects are randomly sampled.
* All levels of factor A are independent.
* All levels of factor B are independent.
* $\epsilon_{ijk}$~N(0,$\sigma^2_{\epsilon}$)

V.

The data was taken from a random sample of "technology workers" from Seattle and San Francisco. By Shapiro-Wilks and Brown-Forstyhe diagnostic testing the dataset is confirmed to hold ANOVA assumption, normality and constant variance. Through test statistic using sum of square error, SSE. Calculated F value which is essentially seeing if there is any significant difference in the SSE of the full model with interaction and reduced model without interaction. With 99% confidence p-value is larger than the confidence level and conclude that the model without interaction effects between profession and region is a statistically better fit than the model with interaction effect. Additionally calculation of a conditional $R^2$, or a conditional percentage of reduction in error showed that profession is more important than region in terms of annual salary. Lastly with 99% confidence pairwise interval comparing factor region explains how significant factor region effects and difference in the group averages is. As a result the best model to conduct Salary data is no interaction effects with factor profession and region effect. 

VI.

Profession and region had no correlation however, both factors affect anaul salary. Generally technology workers in San Francisco tend to earn more money than people in seattle. Also, people who work as a data scientist earn the most software engineerer following next and lastly bioinformatician. Income disparity comes from peoples title rather than where they live.

## Appendix

```{r,eval=FALSE}
h.data = read.csv("/Users/Bosco/Desktop/WINTER2021/STA 106/data/Helicopter.csv")
nt = nrow(h.data)
a=length(unique(h.data$Shift))
h.model = lm(Count ~ Shift, data= h.data)
h.data$ei = h.model$residuals
p.valSW0 = shapiro.test(h.model$residuals)$p.val
suppressMessages(library(car))
the.BFtest = leveneTest(ei~ Shift, data=h.data, center=median)
p.valBF0 = the.BFtest[[3]][1]
t.cutoff= qt(1-0.01,nt -a)
rij = rstandard(h.model)
outliers = which(abs(rij) > t.cutoff)
new.data = h.data[-outliers,]
new.model = lm(Count ~ Shift,data = new.data)
p.valSWN = shapiro.test(new.model$residuals)$p.val
the.BFtest = leveneTest(ei~ Shift, data=new.data, center=median)
p.valBFN = the.BFtest[[3]][1]
suppressMessages(library(EnvStats))
lambda = boxcox(h.model,objective.name = "Shapiro-Wilk",optimize = TRUE)$lambda
newY = (h.data$Count^lambda-1)/(lambda)
T.data = data.frame(Count = newY, Shift = h.data$Shift)
T.model = lm(Count ~ Shift, data= T.data)
T.data$ei = T.model$residuals
p.valSWT = shapiro.test(T.model$residuals)$p.val
the.BFtest = leveneTest(ei~ Shift, data=T.data, center=median)
p.valBFT = the.BFtest[[3]][1]

lambda1 = boxcox(new.model,objective.name = "Shapiro-Wilk",optimize = TRUE)$lambda
newYY = (new.data$Count^lambda1-1)/(lambda1)
TT.data = data.frame(Count = newYY, Shift = new.data$Shift)
TT.model = lm(Count ~ Shift, data= TT.data)
TT.data$ei = TT.model$residuals
p.valSWTT = shapiro.test(TT.model$residuals)$p.val
the.BFtest = leveneTest(ei~ Shift, data=TT.data, center=median)
p.valBFTT = the.BFtest[[3]][1]

SFA.n.pval = anova(new.model)[1,5]
SFA.T.pval = anova(T.model)[1,5]
SFA.TT.pval = anova(TT.model)[1,5]
par(mfrow = c(1, 3))
plot(h.model$fitted.values, h.model$residuals, main = "Errors vs. Group Means",xlab = "Group Means",ylab = "Errors",pch = 19)
abline(h = 0,col = "purple")
qqnorm(h.model$residuals)
qqline(h.model$residuals)
hist(h.model$residuals,main = "Histogram of errors",xlab = "e_ij",pch = 19)
par(mfrow = c(1, 3))
qqnorm(new.model$residuals,main = "Outliers Removed, nt=77")
qqline(new.model$residuals)
qqnorm(T.model$residuals,main = "Transformed data, nt=80")
qqline(T.model$residuals)
qqnorm(TT.model$residuals,main = "Outliers Removed and Transformed data, nt=77",cex.main=0.88)
qqline(TT.model$residuals)
s.data = read.csv("/Users/Bosco/Desktop/WINTER2021/STA 106/data/Salary.csv")
group.means = by(s.data$Annual,s.data$Prof,mean)
group.sds = by(s.data$Annual,s.data$Prof,sd)
group.nis = by(s.data$Annual,s.data$Prof,length)
tthe.summary = rbind(group.means,group.sds,group.nis)
tthe.summary = round(tthe.summary,digits = 4)
colnames(tthe.summary) = names(group.means)
rownames(tthe.summary) = c("Means","Std. Dev","Sample Size")

group.means = by(s.data$Annual,s.data$Region,mean)
group.sds = by(s.data$Annual,s.data$Region,sd)
group.nis = by(s.data$Annual,s.data$Region,length)
the.summary = rbind(group.means,group.sds,group.nis)
the.summary = round(the.summary,digits = 4)
colnames(the.summary) = names(group.means)
rownames(the.summary) = c("Means","Std. Dev","Sample Size")

find.means = function(the.data,fun.name = mean){
a = length(unique(the.data[,2]))
b = length(unique(the.data[,3]))
means.A = by(the.data[,1], the.data[,2], fun.name)
means.B = by(the.data[,1],the.data[,3],fun.name)
means.AB = by(the.data[,1],list(the.data[,2],the.data[,3]),fun.name)
MAB = matrix(means.AB,nrow = b, ncol = a, byrow = TRUE)
colnames(MAB) = names(means.A)
rownames(MAB) = names(means.B)
MA = as.numeric(means.A)
names(MA) = names(means.A)
MB = as.numeric(means.B)
names(MB) = names(means.B)
results = list(A = MA, B = MB, AB = MAB)
return(results)
}
a = length(unique(s.data[,2]))
b = length(unique(s.data[,3]))
n = nrow(s.data)
AB =lm( Annual ~ Prof * Region,data = s.data)
A.B = lm( Annual~  Prof + Region, data = s.data)
A = lm(Annual ~ Prof, data = s.data)
B = lm(Annual ~ Region, data = s.data)
N = lm(Annual ~ 1, data = s.data)
AllM = list(AB, A.B, A, B, N)
SSEs = round(sapply(AllM, function(i) sum(i$residual^2)),2)
SSEF = SSEs[1]
SSER = SSEs[2]
df.F= n - a*b
df.R = n -(a-1)-(b-1)-1
FN = round((SSER - SSEF)/(df.R - df.F),4)
FD = round(SSEF/df.F,4)
Fs = round(FN/FD,4)

results = anova(A.B,AB)
p.val = results[2,6]

s.model = lm(Annual ~ Prof*Region, data= s.data)
s.data$ei = s.model$residuals
p.valSW = shapiro.test(s.model$residuals)$p.val
suppressMessages(library(car))
the.BFtest = leveneTest(ei~ Prof*Region, data=s.data, center=median)
p.valBF = the.BFtest[[3]][1]

Partial.R2 = function(small.model,big.model){
  SSE1 = sum(small.model$residuals^2)
  SSE2 = sum(big.model$residuals^2)
  PR2 = (SSE1 - SSE2)/SSE1
  return(PR2)
}
RAB=Partial.R2(A.B,AB)
RA=Partial.R2(A,A.B)
RB=Partial.R2(B,A.B)
RAO=Partial.R2(N,A)
RBO=Partial.R2(N,B)

scary.CI = function(the.data,MSE,equal.weights = TRUE,multiplier,group,cs){
if(sum(cs) != 0 & sum(cs !=0 ) != 1){
return("Error - you did not input a valid contrast")
}else{
the.means = find.means(the.data)
the.ns =find.means(the.data,length)
nt = nrow(the.data)
a = length(unique(the.data[,2]))
b = length(unique(the.data[,3]))
if(group =="A"){
if(equal.weights == TRUE){
a.means = rowMeans(the.means$AB)
est = sum(a.means*cs)
mul = rowSums(1/the.ns$AB)
SE = sqrt(MSE/b^2 * (sum(cs^2*mul)))
N = names(a.means)[cs!=0]
CS = paste("(",cs[cs!=0],")",sep = "")
fancy = paste(paste(CS,N,sep =""),collapse = "+")
names(est) = fancy
} else{
a.means = the.means$A
est = sum(a.means*cs)
SE = sqrt(MSE*sum(cs^2*(1/the.ns$A)))
N = names(a.means)[cs!=0]
CS = paste("(",cs[cs!=0],")",sep = "")
fancy = paste(paste(CS,N,sep =""),collapse = "+")
names(est) = fancy
}
}else if(group == "B"){
if(equal.weights == TRUE){
b.means = colMeans(the.means$AB)
est = sum(b.means*cs)
mul = colSums(1/the.ns$AB)
SE = sqrt(MSE/a^2 * (sum(cs^2*mul)))
N = names(b.means)[cs!=0]
CS = paste("(",cs[cs!=0],")",sep = "")
fancy = paste(paste(CS,N,sep =""),collapse = "+")
names(est) = fancy
} else{
b.means = the.means$B
est = sum(b.means*cs)
SE = sqrt(MSE*sum(cs^2*(1/the.ns$B)))
N = names(b.means)[cs!=0]
CS = paste("(",cs[cs!=0],")",sep = "")
fancy = paste(paste(CS,N,sep =""),collapse = "+")
names(est) = fancy
}
} else if(group == "AB"){
est = sum(cs*the.means$AB)
SE = sqrt(MSE*sum(cs^2/the.ns$AB))
names(est) = "someAB"
}
the.CI = est + c(-1,1)*multiplier*SE
results = c(est,the.CI)
names(results) = c(names(est),"lower bound","upper bound")
return(results)
}
}
find.mult = function(alpha,a,b,dfSSE,g,group){
if(group == "A"){
Tuk = round(qtukey(1-alpha,a,dfSSE)/sqrt(2),3)
Bon = round(qt(1-alpha/(2*g), dfSSE ) ,3)
Sch = round(sqrt((a-1)*qf(1-alpha, a-1, dfSSE)),3)
}else if(group == "B"){
Tuk = round(qtukey(1-alpha,b,dfSSE)/sqrt(2),3)
Bon = round(qt(1-alpha/(2*g), dfSSE ) ,3)
Sch = round(sqrt((b-1)*qf(1-alpha, b-1, dfSSE)),3)
}else if(group == "AB"){
Tuk = round(qtukey(1-alpha,a*b,dfSSE)/sqrt(2),3)
Bon = round(qt(1-alpha/(2*g), dfSSE ) ,3)
Sch = round(sqrt((a*b-1)*qf(1-alpha, a*b-1, dfSSE)),3)
}
results = c(Bon, Tuk,Sch)
names(results) = c("Bonferroni","Tukey","Scheffe")
return(results)
}
SSE = sum(s.model$residuals^2)
MSE = SSE/(n-a*b)
Tuk = find.mult(alpha = 0.01, a = 3, b = 2, dfSSE = 120 - 2*3, g = 1, group = "B")[2]
B.cs = c(1,-1)
CI=scary.CI(s.data,MSE,equal.weights = FALSE,Tuk,"B",B.cs)[2:3]
suppressMessages(library(gridExtra))
suppressMessages(library(ggplot2))
a=ggplot(s.data, aes(x = Annual)) + geom_histogram(binwidth = 10,color = "black",fill = "white") +
facet_grid(Prof ~.) +ggtitle("Annual by number of Prof")
b=ggplot(s.data, aes(x = Annual)) + geom_histogram(binwidth = 10,color = "black",fill = "white") +
facet_grid(Region ~.) +ggtitle("Annual by number of Region")
grid.arrange(a,b, nrow=1,ncol=2)
par(mfrow = c(2, 2),mar=c(2,0.5,2,0.5))
boxplot(Annual ~ Prof,data=s.data, main="box plot by Prof")
boxplot(Annual ~ Region,data=s.data, main="box plot by Region")
suppressMessages(require(graphics))
interaction.plot(x.factor = s.data$Prof, trace.factor = s.data$Region, response = s.data$Annual, type = "b", ylab = "Annual", xlab = "Prof", pch=c(2,4), col = c(2,4), lty = 1, lwd = 2, trace.label = "Region", xpd = FALSE)
interaction.plot(x.factor = s.data$Region, trace.factor = s.data$Prof, response = s.data$Annual, type = "b", ylab = "Annual", xlab = "Prof", pch=c(2,4), col = c(2,4), lty = 1, lwd = 2, trace.label = "Region", xpd = FALSE)
```


